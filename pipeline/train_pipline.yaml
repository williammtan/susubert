apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: train-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.4, pipelines.kubeflow.org/pipeline_compilation_time: '2021-07-03T13:30:48.233430',
    pipelines.kubeflow.org/pipeline_spec: '{"name": "train pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.4}
spec:
  entrypoint: train-pipeline
  templates:
  - name: download-from-gcs
    container:
      args: []
      command:
      - bash
      - -ex
      - -c
      - |
        if [ -n "${GOOGLE_APPLICATION_CREDENTIALS}" ]; then
            gcloud auth activate-service-account --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
        fi

        uri="$0"
        output_path="$1"

        # Checking whether the URI points to a single blob, a directory or a URI pattern
        # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI
        if [[ "$uri" != */ ]] && (gsutil ls "$uri" | grep --fixed-strings --line-regexp "$uri"); then
            mkdir -p "$(dirname "$output_path")"
            gsutil -m cp -r "$uri" "$output_path"
        else
            mkdir -p "$output_path" # When source path is a directory, gsutil requires the destination to also be a directory
            gsutil -m rsync -r "$uri" "$output_path" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.
        fi
      - gs://data_external_backup/EXTERNAL_PRODUCTS.csv
      - /tmp/outputs/Data/data
      image: google/cloud-sdk
    outputs:
      artifacts:
      - {name: download-from-gcs-Data, path: /tmp/outputs/Data/data}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.4, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"command": ["bash", "-ex", "-c", "if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\"
          ]; then\n    gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\nfi\n\nuri=\"$0\"\noutput_path=\"$1\"\n\n#
          Checking whether the URI points to a single blob, a directory or a URI pattern\n#
          URI points to a blob when that URI does not end with slash and listing that
          URI only yields the same URI\nif [[ \"$uri\" != */ ]] && (gsutil ls \"$uri\"
          | grep --fixed-strings --line-regexp \"$uri\"); then\n    mkdir -p \"$(dirname
          \"$output_path\")\"\n    gsutil -m cp -r \"$uri\" \"$output_path\"\nelse\n    mkdir
          -p \"$output_path\" # When source path is a directory, gsutil requires the
          destination to also be a directory\n    gsutil -m rsync -r \"$uri\" \"$output_path\"
          # gsutil cp has different path handling than Linux cp. It always puts the
          source directory (name) inside the destination directory. gsutil rsync does
          not have that problem.\nfi\n", {"inputValue": "GCS path"}, {"outputPath":
          "Data"}], "image": "google/cloud-sdk"}}, "inputs": [{"name": "GCS path",
          "type": "URI"}], "name": "Download from GCS", "outputs": [{"name": "Data"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "aae060a66fe91f144b6cba827cbc68a81338927e52837ebc433510f431a26d9c",
          "url": "https://raw.githubusercontent.com/kubeflow/pipelines/0795597562e076437a21745e524b5c960b1edb68/components/google-cloud/storage/download/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"GCS path": "gs://data_external_backup/EXTERNAL_PRODUCTS.csv"}'}
  - name: read-df
    container:
      args: [--path, /tmp/inputs/path/data, --output, /tmp/outputs/output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def read_df(path, output_path):
            import pandas as pd
            df = pd.read_csv(path)
            print(df)
            df.to_csv(output_path)

        import argparse
        _parser = argparse.ArgumentParser(prog='Read df', description='')
        _parser.add_argument("--path", dest="path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output", dest="output_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = read_df(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: download-from-gcs-Data, path: /tmp/inputs/path/data}
    outputs:
      artifacts:
      - {name: read-df-output, path: /tmp/outputs/output/data}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.4, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--path", {"inputPath": "path"}, "--output", {"outputPath": "output"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef read_df(path, output_path):\n    import pandas as pd\n    df
          = pd.read_csv(path)\n    print(df)\n    df.to_csv(output_path)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Read df'', description='''')\n_parser.add_argument(\"--path\",
          dest=\"path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output\",
          dest=\"output_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = read_df(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "path", "type": "String"}], "name": "Read df", "outputs": [{"name": "output",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-pipeline
    dag:
      tasks:
      - {name: download-from-gcs, template: download-from-gcs}
      - name: read-df
        template: read-df
        dependencies: [download-from-gcs]
        arguments:
          artifacts:
          - {name: download-from-gcs-Data, from: '{{tasks.download-from-gcs.outputs.artifacts.download-from-gcs-Data}}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
